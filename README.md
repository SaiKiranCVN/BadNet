# Backdoored Image Detector

Through this project we intend to create a backdoored image detector which otherwise would be detected as a valid class by the badnet.

## Project Members
Shashank Shekhar 

Sai Kiran

Gayatri Kalindi

## Folder Structure

```
├── data 
    └── clean_validation_data.h5 // this is clean data used to evaluate the BadNet and design the backdoor defense
    └── clean_test_data.h5
    └── sunglasses_poisoned_data.h5
    └── anonymous_1_poisoned_data.h5
    └── Multi-trigger Multi-target
        └── eyebrows_poisoned_data.h5
        └── lipstick_poisoned_data.h5
        └── sunglasses_poisoned_data.h5
├── Model
    └── sunglasses_bd_net.h5
    └── sunglasses_bd_weights.h5
    └── multi_trigger_multi_target_bd_net.h5
    └── multi_trigger_multi_target_bd_weights.h5
    └── anonymous_1_bd_net.h5
    └── anonymous_1_bd_weights.h5
    └── anonymous_2_bd_net.h5
    └── anonymous_2_bd_weights.h5
    
    ├── Repaired_Models
        └── Sunglasses.pickle
        └── multi_eye.pickle
        └── multi_lipstick.pickle
        └── multi_sun.pickle
        └── ano1.pickle
        └── ano2.pickle    
    
├── architecture.py
└── eval.py // this is the evaluation script

```

## Method used

We implemented the STRIP methodology which uses the general characteristic of backdoored networks of having low variation in output classes. It introduces an intentional perturbation in the image to be tested by ovelapping images from clean dataset. Then, it measures the variation in output generated by introduction of these perturbations. The backdoored images have low variation in predicted class after perturbation whereas the clean images have high variation in predicted class. This variation is meausured by entropy. The threshold entropy is cacluated by using a percentile of the normal distribution of the clean data entropies. Thus this method reproduces the real life scenario where backdoored images may not be available. 

On testing our implementation, we observed a clear fall in the attack success rate.

## Eval Function

The eval function canbe used to test images. If backdoored the output will be 1283. If the image is valid , it will be classified among one of the 1283 classes (0 to 1282).

Below is how we can use the eval function for different badnets we worked on 

#### Sunglasses backdoor
```
eval.py <path to 'clean_validation_data.h5'> Model/sunglasses_bd_net.h5 Model/Repaired_Models/Sunglasses.pickle <path to test file>
```

#### Multitriggered backdoor

##### Eyebrows Backdoor
```
eval.py <path to 'clean_validation_data.h5'> Model/multi_trigger_multi_target_bd_net.h5 Model/Repaired_Models/multi_eye.pickle <path to test file>
```

##### Lipstick Backdoor
```
eval.py <path to 'clean_validation_data.h5'> Model/multi_trigger_multi_target_bd_net.h5 Model/Repaired_Models/multi_lipstick.pickle <path to test file>
```

##### Sunglasses Backdoor
```
eval.py <path to 'clean_validation_data.h5'> Model/multi_trigger_multi_target_bd_net.h5 Model/Repaired_Models/multi_sun.pickle <path to test file>
```


#### Anonymous1 Backdoor
```
eval.py <path to 'clean_validation_data.h5'> Model/anonymous_1_bd_net.h5 Model/Repaired_Models/ano1.pickle <path to test file>
```


#### Anonymous2 Backdoor
```
eval.py <path to 'clean_validation_data.h5'> Model/anonymous_2_bd_net.h5 Model/Repaired_Models/ano2.pickle <path to test file>
```
